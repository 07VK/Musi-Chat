{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vFvZ3HC4Z1nu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJonTSTgZrxV"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers==4.50.3 datasets torch pandas numpy scikit-learn spacy matplotlib plotly ipywidgets tqdm jsonlines --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "FwH2ZESraVAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4e7FqlV7aX6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "XNcZIzU3ayPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust file paths as needed:\n",
        "train_file = '/content/drive/MyDrive/AmazonReviews/train.ft.txt'\n",
        "test_file = '/content/drive/MyDrive/AmazonReviews/test.ft.txt'"
      ],
      "metadata": {
        "id": "3iZOqj1nbCqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# --- Step 1: Load the fastText-formatted dataset more efficiently --- #\n",
        "\n",
        "def load_ft_dataset_efficient(file_path):\n",
        "    \"\"\"\n",
        "    Load a fastText-formatted dataset using a generator for memory efficiency.\n",
        "    \"\"\"\n",
        "    def data_generator(file_path):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(\" \", 1)\n",
        "                if len(parts) < 2:\n",
        "                    continue\n",
        "                label_str = parts[0]\n",
        "                text = parts[1]\n",
        "                label = 0 if label_str == \"__label__1\" else 1\n",
        "                yield {\"text\": text, \"label\": label}\n",
        "\n",
        "    return Dataset.from_generator(data_generator, gen_kwargs={\"file_path\": file_path})\n",
        "\n",
        "print(\"Loading training dataset...\")\n",
        "train_dataset = load_ft_dataset_efficient(train_file)\n",
        "print(\"Number of training samples:\", len(train_dataset))\n",
        "\n",
        "print(\"Loading testing dataset...\")\n",
        "test_dataset = load_ft_dataset_efficient(test_file)\n",
        "print(\"Number of testing samples:\", len(test_dataset))\n",
        "\n",
        "# --- Step 2: Tokenize the Dataset --- #\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Use batched=True for efficiency\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove the 'text' column as the model doesn't need it, keeping only the tokenized inputs\n",
        "tokenized_train = tokenized_train.remove_columns([\"text\"])\n",
        "tokenized_test = tokenized_test.remove_columns([\"text\"])\n",
        "\n",
        "# Rename the 'label' column to 'labels' as expected by the Trainer\n",
        "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
        "tokenized_test = tokenized_test.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Set the format to PyTorch tensors for efficient training\n",
        "tokenized_train.set_format(\"torch\")\n",
        "tokenized_test.set_format(\"torch\")\n",
        "\n",
        "# --- Step 3: Fine-Tune DistilBERT --- #\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Determine the device to use for training\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device) # Move the model to the GPU if available\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/AmazonReviews/distilbert_finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=64,  # Increased batch size significantly\n",
        "    per_device_eval_batch_size=64,   # Increased evaluation batch size\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,                      # Enable mixed precision for faster training and lower memory usage\n",
        "    dataloader_num_workers=4,       # Use multiple workers for data loading (adjust based on your CPU cores)\n",
        "    use_mps_device=True if device.type == 'mps' else False, # Enable MPS for Apple Silicon GPUs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        ")\n",
        "\n",
        "# Begin fine-tuning\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"/content/drive/MyDrive/AmazonReviews/distilbert_finetuned\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/AmazonReviews/distilbert_finetuned\")\n",
        "print(\"Fine-tuning completed and model saved.\")"
      ],
      "metadata": {
        "id": "hFbJCksia-Th"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}