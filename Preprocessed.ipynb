{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install transformers datasets torch pandas numpy scikit-learn spacy matplotlib plotly ipywidgets tqdm"
      ],
      "metadata": {
        "id": "4A2mI7chOEUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxQDnvBFWiFB",
        "outputId": "e4cddcea-d73e-4d01-c5c2-2d0b4e07d337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Download spaCy model\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hysuRk1XWiQu",
        "outputId": "15c09391-8c2e-4182-e001-eede1057639d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uuFdUJua2xz9"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define file paths (adjust to your environment)\n",
        "metadata_path = '/content/drive/MyDrive/AmazonReviews/meta_Musical_Instruments.jsonl'\n",
        "reviews_path = '/content/drive/MyDrive/AmazonReviews/Musical_Instruments.jsonl'\n",
        "output_path = '/content/drive/MyDrive/AmazonReviews/unified.jsonl'\n",
        "\n",
        "# Target number of unique parent_asins\n",
        "target_count = 250000\n",
        "\n",
        "# Data containers\n",
        "unique_parent_asins = set()\n",
        "unified_data = {}  # Dictionary keyed by parent_asin\n",
        "\n",
        "print(\"Processing metadata to collect up to\", target_count, \"unique parent_asins...\")\n",
        "\n",
        "# Stream the metadata file line by line\n",
        "with open(metadata_path, 'r') as meta_file:\n",
        "    for line in meta_file:\n",
        "        try:\n",
        "            record = json.loads(line.strip())\n",
        "        except json.JSONDecodeError:\n",
        "            continue  # Skip malformed lines\n",
        "        parent_asin = record.get('parent_asin')\n",
        "        if parent_asin and parent_asin not in unique_parent_asins:\n",
        "            unique_parent_asins.add(parent_asin)\n",
        "            # Initialize the record with an empty list for reviews\n",
        "            record['reviews'] = []\n",
        "            unified_data[parent_asin] = record\n",
        "\n",
        "            # Print status every 1,000 new unique parent_asins\n",
        "            if len(unique_parent_asins) % 1000 == 0:\n",
        "                print(f\"Collected {len(unique_parent_asins)} unique parent_asins...\")\n",
        "\n",
        "            # Once we reach the target count, stop processing metadata\n",
        "            if len(unique_parent_asins) >= target_count:\n",
        "                break\n",
        "\n",
        "print(\"Metadata processing complete. Total unique parent_asins collected:\", len(unique_parent_asins))\n",
        "\n",
        "print(\"\\nProcessing reviews and merging into unified data...\")\n",
        "\n",
        "# Process the reviews file to add reviews only for products in our unified dataset.\n",
        "reviews_processed = 0\n",
        "with open(reviews_path, 'r') as reviews_file:\n",
        "    for line in reviews_file:\n",
        "        try:\n",
        "            review = json.loads(line.strip())\n",
        "        except json.JSONDecodeError:\n",
        "            continue  # Skip malformed lines\n",
        "\n",
        "        parent_asin = review.get('parent_asin')\n",
        "        if parent_asin and parent_asin in unified_data:\n",
        "            unified_data[parent_asin]['reviews'].append(review)\n",
        "\n",
        "        reviews_processed += 1\n",
        "        # Print status update every 10,000 reviews processed.\n",
        "        if reviews_processed % 10000 == 0:\n",
        "            print(f\"Processed {reviews_processed} reviews...\")\n",
        "\n",
        "print(\"Finished processing reviews.\")\n",
        "\n",
        "# Save the unified dataset (list of products) into a JSON file\n",
        "with open(output_path, 'w') as out_file:\n",
        "    json.dump(list(unified_data.values()), out_file, indent=4)\n",
        "\n",
        "print(f\"\\nUnified dataset with {len(unified_data)} products saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "2FxIdkimjCHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Path to your unified dataset JSON file (the one produced after merging metadata and reviews)\n",
        "input_path = '/content/drive/MyDrive/AmazonReviews/unified.jsonl'\n",
        "output_path = '/content/drive/MyDrive/AmazonReviews/filtered_unified_dataset.json'\n",
        "\n",
        "# Allowed fields for the product metadata\n",
        "meta_allowed = [\n",
        "    \"main_category\", \"title\", \"average_rating\", \"rating_number\",\n",
        "    \"features\", \"description\", \"price\", \"store\", \"categories\", \"details\",\n",
        "    \"parent_asin\", \"reviews\"\n",
        "]\n",
        "\n",
        "# Allowed fields for each review inside the \"reviews\" list\n",
        "review_allowed = [\n",
        "    \"rating\", \"text\", \"user_id\", \"helpful_vote\", \"verified_purchase\"\n",
        "]\n",
        "\n",
        "def filter_review(review):\n",
        "    \"\"\"Return a new dictionary containing only the allowed review fields.\"\"\"\n",
        "    return { key: review[key] for key in review_allowed if key in review }\n",
        "\n",
        "def filter_meta(record):\n",
        "    \"\"\"Return a new dictionary for a product record containing only allowed metadata fields.\n",
        "       Also, filter each review in the reviews list.\"\"\"\n",
        "    new_record = { key: record.get(key) for key in meta_allowed }\n",
        "    # If 'reviews' exists and is a list, filter each review's keys.\n",
        "    if new_record.get(\"reviews\") is not None and isinstance(new_record[\"reviews\"], list):\n",
        "        new_record[\"reviews\"] = [ filter_review(r) for r in new_record[\"reviews\"] ]\n",
        "    return new_record\n",
        "\n",
        "# Load the unified dataset\n",
        "with open(input_path, 'r') as f:\n",
        "    unified_data = json.load(f)\n",
        "\n",
        "# Process each product record to retain only the needed fields\n",
        "filtered_data = [ filter_meta(record) for record in unified_data ]\n",
        "\n",
        "# Save the filtered dataset into a new JSON file\n",
        "with open(output_path, 'w') as out_f:\n",
        "    json.dump(filtered_data, out_f, indent=4)\n",
        "\n",
        "print(f\"Filtered dataset saved to: {output_path}\")"
      ],
      "metadata": {
        "id": "eNXFYvrajE_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Minimal cleaning function: remove URLs, HTML tags, extra whitespace, and lowercase the text.\n",
        "def minimal_clean_text(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)      # Remove HTML tags\n",
        "    text = re.sub(r\"\\s+\", \" \", text)       # Normalize whitespace\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Simple sentiment labeling based on rating.\n",
        "def label_sentiment(rating):\n",
        "    if rating is None:\n",
        "        return None\n",
        "    if rating <= 2.0:\n",
        "        return \"negative\"\n",
        "    elif rating == 3.0:\n",
        "        return \"neutral\"\n",
        "    else:  # rating >= 4.0\n",
        "        return \"positive\"\n",
        "\n",
        "# File paths (adjust these as needed)\n",
        "input_path = '/content/drive/MyDrive/AmazonReviews/filtered_unified_dataset.json'\n",
        "output_path = '/content/drive/MyDrive/AmazonReviews/cfu_dataset.json' #clean_filtered_unified dataset\n",
        "\n",
        "# Load the unified dataset (each record represents a product)\n",
        "with open(input_path, 'r') as f:\n",
        "    unified_data = json.load(f)\n",
        "\n",
        "filtered_data = []\n",
        "product_record_counter = 1\n",
        "\n",
        "# Process each product record\n",
        "for product in unified_data:\n",
        "    # Extract product-level fields\n",
        "    main_category = product.get(\"main_category\")\n",
        "    title = product.get(\"title\")\n",
        "    parent_asin = product.get(\"parent_asin\")\n",
        "    average_rating = product.get(\"average_rating\")\n",
        "    overall_sentiment = label_sentiment(average_rating)  # Compute overall sentiment from average_rating\n",
        "    rating_number = product.get(\"rating_number\")\n",
        "    features = product.get(\"features\")\n",
        "    description = product.get(\"description\")\n",
        "    details = product.get(\"details\")\n",
        "    price = product.get(\"price\")\n",
        "    store = product.get(\"store\")\n",
        "    categories = product.get(\"categories\")\n",
        "\n",
        "    # Process the reviews list: for each review, keep only the desired fields.\n",
        "    reviews = product.get(\"reviews\", [])\n",
        "    processed_reviews = []\n",
        "    for review in reviews:\n",
        "        original_text = review.get(\"text\", \"\")\n",
        "        cleaned_text = minimal_clean_text(original_text)\n",
        "        review_rating = review.get(\"rating\")\n",
        "        review_sentiment = label_sentiment(review_rating)\n",
        "\n",
        "        new_review = {\n",
        "            \"rating\": review_rating,\n",
        "            \"sentiment\": review_sentiment,\n",
        "            \"text\": original_text,\n",
        "            \"cleaned_text\": cleaned_text,\n",
        "            \"user_id\": review.get(\"user_id\"),\n",
        "            \"helpful_vote\": review.get(\"helpful_vote\"),\n",
        "            \"verified_purchase\": review.get(\"verified_purchase\")\n",
        "        }\n",
        "        processed_reviews.append(new_review)\n",
        "\n",
        "    # Create a new product record with fields in the specified order and include product_record.\n",
        "    new_product = {\n",
        "        \"product_record\": product_record_counter,  # new sequential field\n",
        "        \"main_category\": main_category,\n",
        "        \"title\": title,\n",
        "        \"parent_asin\": parent_asin,\n",
        "        \"average_rating\": average_rating,\n",
        "        \"overall_sentiment\": overall_sentiment,\n",
        "        \"rating_number\": rating_number,\n",
        "        \"features\": features,\n",
        "        \"description\": description,\n",
        "        \"details\": details,\n",
        "        \"price\": price,\n",
        "        \"store\": store,\n",
        "        \"categories\": categories,\n",
        "        \"reviews\": processed_reviews\n",
        "    }\n",
        "\n",
        "    filtered_data.append(new_product)\n",
        "\n",
        "    # Print dynamic status update for the processed product.\n",
        "    print(f\"Sample-{product_record_counter} product_record - {product_record_counter} is completed\")\n",
        "\n",
        "    product_record_counter += 1\n",
        "\n",
        "# Save the filtered dataset to a new JSON file with pretty-printing\n",
        "with open(output_path, 'w') as out_file:\n",
        "    json.dump(filtered_data, out_file, indent=4)\n",
        "\n",
        "print(f\"\\nFiltered unified dataset saved to: {output_path}\")"
      ],
      "metadata": {
        "id": "N5jPDB4UNzSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_entities(text):\n",
        "    \"\"\"\n",
        "    Extract named entities from the provided text using spaCy.\n",
        "    Returns a list of tuples: [(entity_text, entity_label), ...]\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "def minimal_clean_text(text):\n",
        "    \"\"\"\n",
        "    Perform minimal cleaning: remove URLs, HTML tags, normalize spaces, and lowercase the text.\n",
        "    \"\"\"\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)      # remove HTML tags\n",
        "    text = re.sub(r\"\\s+\", \" \", text)       # normalize whitespace\n",
        "    return text.strip().lower()\n",
        "\n",
        "# File paths (adjust these paths as needed)\n",
        "input_path =   '/content/drive/MyDrive/AmazonReviews/cfu_dataset.json' # Unified dataset from previous merging steps\n",
        "output_path = '/content/drive/MyDrive/AmazonReviews/pcfu_dataset.json' # after preprocessed with NER\n",
        "\n",
        "# Load the unified dataset (assumed to be a list of product records)\n",
        "with open(input_path, 'r') as f:\n",
        "    unified_data = json.load(f)\n",
        "\n",
        "# Process each product record to extract named entities\n",
        "for index, product in enumerate(unified_data):\n",
        "    combined_text = \"\"\n",
        "    # Combine key product fields: title, description, and details\n",
        "    if product.get(\"title\"):\n",
        "        combined_text += product[\"title\"] + \" \"\n",
        "\n",
        "    if product.get(\"description\"):\n",
        "        if isinstance(product[\"description\"], list):\n",
        "            combined_text += \" \".join(product[\"description\"]) + \" \"\n",
        "        else:\n",
        "            combined_text += product[\"description\"] + \" \"\n",
        "\n",
        "    if product.get(\"details\") and isinstance(product[\"details\"], dict):\n",
        "        details_text = \" \".join(str(val) for val in product[\"details\"].values())\n",
        "        combined_text += details_text + \" \"\n",
        "\n",
        "    # Optionally, you can also include aggregated review text if desired.\n",
        "    # For now, we'll only process the product-level fields.\n",
        "\n",
        "    # Apply minimal cleaning before extracting entities (if necessary)\n",
        "    combined_text_cleaned = minimal_clean_text(combined_text)\n",
        "\n",
        "    # Extract entities using spaCy's NER\n",
        "    product[\"named_entities\"] = extract_entities(combined_text_cleaned)\n",
        "\n",
        "    # Print status for this product (sample)\n",
        "    print(f\"Sample-{index + 1} product_record - {index + 1} is completed\")\n",
        "\n",
        "# Save the updated dataset with named_entities back to JSON\n",
        "with open(output_path, 'w') as out_file:\n",
        "    json.dump(unified_data, out_file, indent=4)\n",
        "\n",
        "print(f\"\\nAll samples processed. Updated dataset saved to: {output_path}\")"
      ],
      "metadata": {
        "id": "46BLLUXJNtCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Replace with the actual path where your fine-tuned model is saved.\n",
        "model_path = \"/content/drive/MyDrive/AmazonReviews/distilbert_finetuned\"\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\n",
        "\n",
        "# Test the model on a sample review text\n",
        "sample_text = \"The product was received incomplete. I was supposed to received the full drum set as shown in the picture without the cymbals. But only one box was received with the bass drum and one tom\"\n",
        "result = sentiment_pipeline(sample_text)[0]\n",
        "print(f\"Sentiment: {'positive' if result['label'] == 'LABEL_1' else 'negative'} (score: {result['score']:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZtzDNN6-3xM",
        "outputId": "15bcc3a3-e91e-4a42-d36f-5a25e00e2c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: negative (score: 0.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import pipeline\n",
        "\n",
        "# Paths to input and output JSON files (adjust as needed)\n",
        "input_json_path = '/content/drive/MyDrive/AmazonReviews/pcfu_dataset.json'  # Unified dataset (filtered, cleaned, preprocessed)\n",
        "output_json_path = '/content/drive/MyDrive/AmazonReviews/final_dataset.json'  # Final updated dataset\n",
        "\n",
        "# Load the unified dataset\n",
        "with open(input_json_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Load the fine-tuned DistilBERT sentiment pipeline.\n",
        "# Ensure that model_path points to your fine-tuned model directory.\n",
        "model_path = \"/content/drive/MyDrive/AmazonReviews/distilbert_finetuned\"\n",
        "bert_pipeline = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\n",
        "\n",
        "# Initialize counters for evaluation\n",
        "total_reviews = 0\n",
        "correct_predictions = 0\n",
        "positive_count = 0\n",
        "negative_count = 0\n",
        "product_record_counter = 1\n",
        "\n",
        "# Process each product in the dataset using batch processing for reviews\n",
        "for product in data:\n",
        "    if \"reviews\" in product:\n",
        "        reviews = product[\"reviews\"]\n",
        "        texts = []         # List of cleaned text for valid reviews\n",
        "        valid_indices = [] # Indices corresponding to reviews with valid text\n",
        "\n",
        "        for idx, review in enumerate(reviews):\n",
        "            # Use the \"cleaned_text\" for prediction (if available; otherwise skip this review)\n",
        "            review_text = review.get(\"cleaned_text\", \"\")\n",
        "            if review_text:\n",
        "                texts.append(review_text)\n",
        "                valid_indices.append(idx)\n",
        "            total_reviews += 1\n",
        "\n",
        "        # Process reviews in batch if any valid texts exist\n",
        "        if texts:\n",
        "            # Pass a list of texts; the pipeline will process in batch\n",
        "            predictions = bert_pipeline(texts, truncation=True, max_length=512)\n",
        "            # Assign predictions to the corresponding reviews based on their stored index\n",
        "            for i, idx in enumerate(valid_indices):\n",
        "                result = predictions[i]\n",
        "                # Depending on your fine-tuned model, label may be 'LABEL_1' or 'POSITIVE'\n",
        "                predicted_label = \"positive\" if result[\"label\"] in [\"LABEL_1\", \"POSITIVE\"] else \"negative\"\n",
        "                score = result[\"score\"]\n",
        "                review = reviews[idx]\n",
        "                review[\"bert_sentiment\"] = {\"label\": predicted_label, \"score\": score}\n",
        "\n",
        "                # Remove the 'text' field to reduce file size (if present)\n",
        "                if \"text\" in review:\n",
        "                    del review[\"text\"]\n",
        "\n",
        "                # Compare predicted sentiment with the original sentiment (derived from rating)\n",
        "                original_sentiment = review.get(\"sentiment\", \"\")\n",
        "                if original_sentiment == predicted_label:\n",
        "                    correct_predictions += 1\n",
        "                # Count the sentiment predictions\n",
        "                if predicted_label == \"positive\":\n",
        "                    positive_count += 1\n",
        "                else:\n",
        "                    negative_count += 1\n",
        "\n",
        "    # Print dynamic status update for each processed product\n",
        "    print(f\"Sample product_record - {product_record_counter} is completed\")\n",
        "    product_record_counter += 1\n",
        "\n",
        "# Compute overall accuracy (if at least one review was processed)\n",
        "accuracy = correct_predictions / total_reviews if total_reviews > 0 else 0.0\n",
        "\n",
        "# Save the updated dataset to a new JSON file with pretty printing\n",
        "with open(output_json_path, 'w') as out_f:\n",
        "    json.dump(data, out_f, indent=4)\n",
        "\n",
        "print(f\"\\nTotal reviews processed: {total_reviews}\")\n",
        "print(f\"Accuracy of bert_sentiment compared to original sentiment: {accuracy:.2f}\")\n",
        "print(f\"Count of positive reviews (predicted by BERT): {positive_count}\")\n",
        "print(f\"Count of negative reviews (predicted by BERT): {negative_count}\")\n",
        "print(f\"Updated dataset saved to: {output_json_path}\")"
      ],
      "metadata": {
        "id": "QztTUYnpNoYd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}